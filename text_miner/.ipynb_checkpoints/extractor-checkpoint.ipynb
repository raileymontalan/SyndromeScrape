{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, pprint, spacy, string\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from dateutil import parser\n",
    "from jellyfish import jaro_winkler\n",
    "from spacy import *\n",
    "from spacy.pipeline import *\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load trained model, data, and psgc\n",
    "nlp = spacy.load('models')\n",
    "df = pd.read_json('test_data.json')\n",
    "psgc = pd.read_csv('psgc.csv', encoding='latin1')\n",
    "pp = pprint.PrettyPrinter(compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_to_json(record, f):\n",
    "    if record == 'incident':\n",
    "        return {\n",
    "            'disease': str(f[0]),\n",
    "            'date-start': str(f[1]),\n",
    "            'date-end': str(f[2]),\n",
    "            'title': str(f[3]),\n",
    "            'url': str(f[4]),\n",
    "            'geocode': str(f[5]),\n",
    "            'incident': str(f[6])\n",
    "        }\n",
    "    elif record == 'state':\n",
    "        return {\n",
    "            'disease': str(f[0]),\n",
    "            'date-start': str(f[1]),\n",
    "            'date-end': str(f[2]),\n",
    "            'title': str(f[3]),\n",
    "            'url': str(f[4]),\n",
    "            'geocode': str(f[5]),\n",
    "            'state': str(f[6])\n",
    "        }\n",
    "    elif record == 'trend':\n",
    "        return {\n",
    "            'disease': str(f[0]),\n",
    "            'date-start': str(f[1]),\n",
    "            'date-end': str(f[2]),\n",
    "            'title': str(f[3]),\n",
    "            'url': str(f[4]),\n",
    "            'geocode': str(f[5]),\n",
    "            'trend': str(f[6])\n",
    "        }\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of tuples composed of the name\n",
    "# and type of the location\n",
    "\n",
    "def extract_location(text, ref=False):\n",
    "    locs = []\n",
    "    \n",
    "    # Retrieves the location specified at the start of each article\n",
    "    if ref:\n",
    "        for word in text:\n",
    "            if word.is_punct:\n",
    "                continue\n",
    "            elif word.ent_type_ == 'GPE':\n",
    "                locs.append(word)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    # Retrieves the location found in any given sentence\n",
    "    else:\n",
    "        for word in text:\n",
    "            if word not in STOP_WORDS:\n",
    "                if word.ent_type_ == 'GPE':\n",
    "                    locs.append(word)\n",
    "    \n",
    "    location = []\n",
    "    [location.append((word.text, 'none')) for word in locs]\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(location):\n",
    "    ps = deepcopy(psgc)\n",
    "    \n",
    "    # sort list by location type from most general to most specific, w/ 'none' at the end\n",
    "    loc_types = ['reg', 'prov', 'mun', 'bgy', 'none']\n",
    "    loc_sorter = [loc_types.index(tup[1]) for tup in location]\n",
    "    location = [x for _, x in sorted(zip(loc_sorter, location))]\n",
    "\n",
    "    # list of row indices of possible locations\n",
    "    possible_locs = [x for x in range(42026)]\n",
    "    cols = ['Region', 'Province', 'Municipality', 'Barangays']\n",
    "    col = 0\n",
    "    \n",
    "    # narrow down list of possible locations, starting from general to specific\n",
    "    for tup in location:\n",
    "        # first, deal with all locations that do not have a 'none' type\n",
    "        if not tup[1] == 'none':\n",
    "            ps = ps.iloc[possible_locs]\n",
    "            col = 'Region'\n",
    "\n",
    "            if tup[1] == 'prov':\n",
    "                col = 'Province'\n",
    "            elif tup[1] == 'mun':\n",
    "                col = 'Municipality'\n",
    "            elif tup[1] == 'bgy':\n",
    "                col = 'Barangays'\n",
    "            \n",
    "            # substring search\n",
    "            possible_locs = [i for i,row in ps.iterrows() if tup[0].lower() in row[col].lower()]\n",
    "            col = ps.columns.get_loc(col)\n",
    "            \n",
    "        # next, deal with 'none' loc_types OR zero matches from previous search\n",
    "        if tup[1] == 'none' or len(possible_locs) == 0:\n",
    "            sub_possible_locs = []\n",
    "            \n",
    "            # search all columns for all matches per tuple\n",
    "            for x in cols:\n",
    "                sub_possible_locs += [i for i,row in ps.iterrows() if tup[0].lower() in row[x].lower()]\n",
    "            \n",
    "            possible_locs = list(set(possible_locs) & set(sub_possible_locs))\n",
    "    \n",
    "    # still dealing with 'none' loc_types and/or zero matches from previous search\n",
    "    # keep track of column where most specific area is found\n",
    "    for tup in location:\n",
    "        if tup[1] == 'none':\n",
    "            for loc in possible_locs:\n",
    "                for x in cols:\n",
    "                    if (tup[0].lower() in ps.iloc[loc, ps.columns.get_loc(x)].lower()) and (ps.columns.get_loc(x) > col):\n",
    "                        col = ps.columns.get_loc(x)\n",
    "    \n",
    "    col += 1\n",
    "\n",
    "    # get possible area codes\n",
    "    # area_codes keeps the first instance row index of each unique area code\n",
    "    new_locs = []\n",
    "    area_codes = []\n",
    "    \n",
    "    for row in possible_locs:\n",
    "        if not psgc.iloc[row, col] in new_locs:\n",
    "            new_locs.append(psgc.iloc[row, col])\n",
    "            area_codes.append((row, psgc.iloc[row, col]))\n",
    "    \n",
    "    print(area_codes)\n",
    "    \n",
    "    # if more than one possible area, do similarity check and choose area w/ highest similarity (jaro-winkler distance)\n",
    "    for tup in location:\n",
    "        if len(area_codes) != 1:\n",
    "            # deal first with locations that do not have a 'none' type\n",
    "            if not tup[1] == 'none':\n",
    "                col = ps.columns.get_loc('Region')\n",
    "\n",
    "                if tup[1] == 'prov':\n",
    "                    col = ps.columns.get_loc('Province')\n",
    "                elif tup[1] == 'mun':\n",
    "                    col = ps.columns.get_loc('Municipality')\n",
    "                elif tup[1] == 'bgy':\n",
    "                    col = ps.columns.get_loc('Barangays')\n",
    "                \n",
    "                jaro_distance = [jaro_winkler(tup[0].lower(), psgc.iloc[area[0], col].lower()) for area in area_codes]\n",
    "        \n",
    "        # return None if no area code found\n",
    "        if not area_codes:\n",
    "            return None\n",
    "        # else, return area code\n",
    "        else:\n",
    "            return area_codes[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "features:\n",
    "    disease\n",
    "    date\n",
    "    title\n",
    "    url\n",
    "    geocode\n",
    "    count\n",
    "\"\"\"\n",
    "def extract_incident(sent, refs):\n",
    "    # Load metadata values\n",
    "    dis, date, title, url, loc = refs\n",
    "    date_start = date.strftime('%Y-%m-%d')\n",
    "    date_end = date.strftime('%Y-%m-%d')\n",
    "    relations = []\n",
    "    \n",
    "    extracted_loc = extract_location(sent) # Look for location specified in the sentence\n",
    "    #if extracted_loc:\n",
    "    #    loc = search(extracted_loc)\n",
    "        \n",
    "    # Find all CARDINAL entities, then check if they are \n",
    "    # linked to a disease, case, or location\n",
    "    # If they are, they are probably incidence counts\n",
    "    for number in filter(lambda w: w.ent_type_ == 'CARDINAL', sent):\n",
    "        if number.dep_ in ('attr', 'dobj'):\n",
    "            case = [w for w in number.head.lefts if w.ent_type == 'nsubj']\n",
    "            if case: \n",
    "                count = number.text.replace(',', '')\n",
    "                relations.append(record_to_json('incident', [dis,  date_start, date_end, title, url, loc, count]))\n",
    "        else:\n",
    "            case = number.head.ent_type_\n",
    "            count = number.text.replace(',', '')\n",
    "            if case == 'CASE':\n",
    "                relations.append(record_to_json('incident', [dis,  date_start, date_end, title, url, loc, count]))\n",
    "            if case == 'LOC':\n",
    "                relations.append(record_to_json('incident', [dis,  date_start, date_end, title, url, loc, count]))\n",
    "            \n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "features:\n",
    "    disease\n",
    "    date\n",
    "    title\n",
    "    url\n",
    "    geocode\n",
    "    state\n",
    "\"\"\"\n",
    "def extract_status(sent, refs):\n",
    "    # Load metadata values\n",
    "    dis, date, title, url, loc = refs\n",
    "    date_start = date.strftime('%Y-%m-%d')\n",
    "    date_end = date.strftime('%Y-%m-%d')\n",
    "    relations = []\n",
    "    \n",
    "    extracted_loc = extract_location(sent) # Look for location specified in the sentence\n",
    "    #if extracted_loc:\n",
    "    #    loc = search(extracted_loc)\n",
    "    \n",
    "    ## I need to find a way to locate a substring of STATE tokens,\n",
    "    ## concatenate the token.text of each token into one string\n",
    "    ## then append it into the relations list\n",
    "    \n",
    "    ## this one here doesn't implement that yet vvv\n",
    "    states = filter(lambda x: x.ent_type_ == 'STATE', sent)\n",
    "    state = ' '.join(map(str, states))\n",
    "    if state: relations.append(record_to_json('state', [dis, date_start, date_end, title, url, loc, state]))\n",
    "        \n",
    "    return relations    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "features:\n",
    "    disease\n",
    "    date\n",
    "    title\n",
    "    url\n",
    "    geocode\n",
    "    change\n",
    "\"\"\"\n",
    "def extract_trend(sent, refs):\n",
    "    # Load metadata values\n",
    "    dis, date, title, url, loc = refs\n",
    "    date_start = date.strftime('%Y-%m-%d')\n",
    "    date_end = date.strftime('%Y-%m-%d')\n",
    "    relations = []\n",
    "    \n",
    "    extracted_loc = extract_location(sent) # Look for location specified in the sentence\n",
    "    #if extracted_loc:\n",
    "    #    loc = search(extracted_loc)\n",
    "    \n",
    "    for change in filter(lambda w: w.ent_type_ == 'CHANGE', sent):\n",
    "        for child in change.children:\n",
    "            if child.ent_type_ == 'PERCENT':\n",
    "                relations.append(record_to_json('trend', [dis, date_start, date_end, title, url, loc, change]))\n",
    "                \n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_refs(article):\n",
    "    dis = article['disease']\n",
    "    date = article['timestamp'] \n",
    "    title = article['title']\n",
    "    url = article['url']\n",
    "    loc = search(extract_location(nlp(article['content']), ref=True))\n",
    "    #loc = extract_location(nlp(article['content']), ref=True)\n",
    "    return [dis, date, title, url, loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def extract(df):\n",
    "    total_incidents = []\n",
    "    total_statuses = []\n",
    "    total_trends = []\n",
    "    for index, article in df.iterrows():\n",
    "        doc = nlp(article['content'])\n",
    "        refs = extract_refs(article)\n",
    "        deets = [article['title'], article['url']]\n",
    "        incidents = []\n",
    "        statuses = []\n",
    "        trends = []\n",
    "        for sent in doc.sents:\n",
    "            i = extract_incident(sent,  refs)\n",
    "            s = extract_status(sent, refs)\n",
    "            t = extract_trend(sent, refs)\n",
    "            if i: [incidents.append(x) for x in i]\n",
    "            if s: [statuses.append(x) for x in s]\n",
    "            if t: [trends.append(x) for x in t]\n",
    "        if incidents: pp.pprint(incidents)\n",
    "        #if statuses: pp.print(statuses)\n",
    "        #if trends: pp.print(trends)\n",
    "    \"\"\"\n",
    "        if incidents: [total_incidents.append(y) for y in incidents]\n",
    "        if statuses: [total_statuses.append(y) for y in statuses]\n",
    "        if trends: [total_trends.append(y) for y in trends]\n",
    "    \n",
    "    with open('incidents.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(total_incidents, outfile, indent=4, ensure_ascii=False)\n",
    "    with open('statuses.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(total_statuses, outfile, indent=4, ensure_ascii=False)\n",
    "    with open('trends.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(total_trends, outfile, indent=4, ensure_ascii=False)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"400\" height=\"224.5\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">ISABELA,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Philippines –</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M225.0,91.5 L233.0,79.5 217.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\"\"\"\n",
    "test = nlp(df['content'][0])\n",
    "sents = [sent.as_doc() for sent in test.sents]\n",
    "displacy.render(sents[0], style='dep', jupyter=True)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "a = nlp(df['content'][7])\n",
    "displacy.render(a, style='ent', jupyter=True)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
